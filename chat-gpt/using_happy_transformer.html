<!doctype html>
<html lang='en'>
<head>
<meta charset='utf-8' name='viewport' content='width=device-width, initial-scale=1.0'>
<title>using happy transformer . Today I Learned (secretGeek)</title>
<script type="text/javascript" src="/highlight.pack.js" defer></script>
<script type="text/javascript" src="/script.js" defer></script>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’¡</text></svg>">
</head>
<body>
<link rel="stylesheet" type="text/css" href="/highlight.min.css">
<link rel="stylesheet" type="text/css" href="/dracula.css">
<link rel="stylesheet" type="text/css" href="/style.css">
<header>
<div class='nav'><a href='/'>ðŸ’¡ Today I Learned</a>
<form action="https://www.google.com/search" method="get" class='search'>
<input type="hidden" value="til.secretgeek.net" name="as_sitesearch">
<input type="submit" value="ðŸ”Ž" class="search-button" name="btnG" title="Search" style="float:right">
<input type="text" maxlength="255" size="20" name="q" class="search-text" placeholder="Search..." style="float:right">
</form>
</div>
</header>
<div id='breadcrumb' class='breadcrumb'>*</div>
<article>
<h1 id="using-happytransformer-for-text-generation-and-more">Using HappyTransformer for Text Generation and More</h1>
<p>Install HappyTransformer</p>
<p>(From an <strong>elevated</strong> / admin shell) (With python 3, and <a href="../python/pip.html">pip</a>, available)</p>
<pre><code class="language-python">pip install happytransformer
</code></pre>
<h2 id="first-program">First Program</h2>
<p>This program will download the model 'EleutherAI/gpt-neo-1.3B' which is a few gigabytes, and takes a while to download (the first time only)</p>
<p>And we will ask it to generate text that continues on from our prompt of &quot;To make a cheese sandwich &quot;.</p>
<p>The result will be very repetitive...</p>
<p>(Save this in a <code>my-first-script.py</code> python file. Execute it with <code>&gt; python my-first-script.py</code>)</p>
<pre><code class="language-python">from happytransformer import HappyGeneration

# first time you run this it will take a long time to download the model 'EleutherAI/gpt-neo-1.3B' which is a few gigabytes.
happy_gen = HappyGeneration(&quot;GPT-NEO&quot;, &quot;EleutherAI/gpt-neo-1.3B&quot;)

# as we provided no other arguments, the output will be very repetitive
result = happy_gen.generate_text(&quot;To make a cheese sandwich &quot;)
print(result)
print(result.text)
</code></pre>
<p>And will probably not get you any closer to making a cheese sandwich.</p>
<h2 id="using-arguments-to-refine-our-text-generation">Using arguments to Refine Our Text Generation</h2>
<p>We need to provide more arguments to the <code>generate_text</code> method. It has a parameter. <code>args</code> that takes an object of type <code>GENSettings</code>.</p>
<p>Let's see how to use that...</p>
<h3 id="less-repetitive">Less repetitive</h3>
<p>By providing an argument with the parameter <code>no_repeat_ngram_size=2</code> it means than any chunk of two or more tokens won't be <em>repeated</em> in the output.</p>
<pre><code class="language-python">from happytransformer import HappyGeneration, GENSettings

generation_args = GENSettings(no_repeat_ngram_size=2)
happy_gen = HappyGeneration(&quot;GPT-NEO&quot;, &quot;EleutherAI/gpt-neo-1.3B&quot;)

result = happy_gen.generate_text(&quot;To make a cheese sandwich &quot;, args=generation_args)
print(result)
print(result.text)
</code></pre>
<p><code>Jargon</code> -- I haven't described what a &quot;token&quot; is yet -- for now, consider it to be like &quot;a word&quot; though sometimes it's a fragment of a word (...for common fragments). We can see <em>exactly</em> what a token is when we look at <code>tokenization</code>, or when we look at <a href="training_a_gpt.html">training our GPT</a>.</p>
<h3 id="knobs-and-dials-for-you-to-explore">Knobs and Dials for you to explore</h3>
<p>This time we set more than just the <code>no_repeat_ngram_size</code> parameter.</p>
<pre><code class="language-python">from happytransformer import HappyGeneration, GENSettings

# We set `no_repeat_ngram_size`, `do_sample`, `early_stopping`, `top_k`, `temperature`
generation_args = GENSettings(no_repeat_ngram_size=2, do_sample=True, early_stopping=False, top_k=50, temperature=0.7)
happy_gen = HappyGeneration(&quot;GPT-NEO&quot;, &quot;EleutherAI/gpt-neo-1.3B&quot;)

result = happy_gen.generate_text(&quot;To make a cheese sandwich  &quot;, args=generation_args)
print(result)
print(result.text)
</code></pre>
<p>Here are all the parameters for <code>GENSettings</code> (from <a href="https://happytransformer.com/text-generation/settings/">Happy Transformer documentation</a>)</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>min_length</code></td>
<td>10</td>
<td>Minimum number of generated tokens</td>
</tr>
<tr>
<td><code>max_length</code></td>
<td>50</td>
<td>Maximum number of generated tokens</td>
</tr>
<tr>
<td><code>do_sample</code></td>
<td>False</td>
<td>When True, picks words based on their conditional probability</td>
</tr>
<tr>
<td><code>early_stopping</code></td>
<td>False</td>
<td>When True, generation finishes if the <code>EOS</code> (end of stream) token is reached</td>
</tr>
<tr>
<td><code>num_beams</code></td>
<td>1</td>
<td>Number of steps for each search path</td>
</tr>
<tr>
<td><code>temperature</code></td>
<td>1.0</td>
<td>How sensitive the algorithm is to selecting low probability options</td>
</tr>
<tr>
<td><code>top_k</code></td>
<td>50</td>
<td>How many potential answers are considered when performing sampling</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td>1.0</td>
<td>Min number of tokens are selected where their probabilities add up to <code>top_p</code></td>
</tr>
<tr>
<td><code>no_repeat_ngram_size</code></td>
<td>0</td>
<td>The size of an n-gram that cannot occur more than once. Note <code>0=infinity</code> (I've determined that this is not true for one's bank balance, btw.)</td>
</tr>
<tr>
<td><code>bad_words</code></td>
<td>None</td>
<td>List of words/phrases that cannot be generated.</td>
</tr>
</tbody>
</table>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://happytransformer.com/text-generation/settings/">Happy Transformer Text Generation Settings</a></li>
<li><a href="using_happy_transformer.html">Using HappyTransformer for Text Generation and More</a></li>
<li><a href="../python/pip.html">PIP: Pip installs Packages</a></li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="training_a_gpt.html">Training a GPT-Based Large Language Model (e.g GPT-Neo)</a></li>
</ul>

</article>
<footer><hr /><small><a href='http://secretgeek.net' class='no-glyph'>secretgeek.net</a> | <a href='https://nimbletext.com' class='no-glyph'>nimbletext</a> | <a href='http://timesnapper.com' class='no-glyph'>TimeSnapper</a>|  <a rel='me' href='https://mastodon.cloud/@secretgeek' title='@secretgeek on mastodon' class='no-glyph'>@secretgeek</a> </small><br /><br /></footer>
</body>
</html>